{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to GitHub repository [here](https://github.com/jesp9435/ComSocSci)\n",
    "\n",
    "Group member contributions: Both group members contributed equally to the parts of the assignment. We have worked collaboratively on all parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we got a total of 1484 unique researchers. \n",
    "\n",
    "First we inspected the HTML page, and used the tool to select an element on a page. Using this, it was possible to see a pattern on the website, e.g. that all the conferences were nested under a single class. We thought looking for this would yield the max amount of authors. However, it did not consider some of the names above the nested class . Thus we realized that by searching for \"i\" as in italic, it was possible to retrieve all names on the website. Some names had a title \"Chair\" next to their name, so this was removed after printing the full list of the names. We retrieved data about all the names using the Openalex API, and saved it to a authors.txt. There were duplicates in the results , but we chose to include everyone except if they had no results. In total we scraped results for 3902 people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import json\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK) \n",
    "soup = BeautifulSoup(r.content) \n",
    "parallel_talks = soup.find_all('i')\n",
    "pep=[]\n",
    "for people in parallel_talks:\n",
    "    people_list=people.text.split(\",\")\n",
    "    for i in range(len(people_list)):\n",
    "        pep.append(people_list[i].replace(\"Chair: \",\"\").strip())\n",
    "res=[]\n",
    "[res.append(x) for x in pep if x not in res]\n",
    "count=0\n",
    "authors=[]\n",
    "file1 = open('authors.txt', mode='a', encoding='utf-8')\n",
    "for author in res:\n",
    "    api_string=\"https://api.openalex.org/autocomplete/authors?q=\"\n",
    "    temp_name=author.split(\" \")\n",
    "    api_string+=temp_name[0]+\"%20\"\n",
    "    temp_name.pop(0)\n",
    "    for last_name in temp_name:\n",
    "        api_string+=last_name\n",
    "    r = requests.get(api_string)\n",
    "    soup = BeautifulSoup(r.content) \n",
    "    meta_data=''.join(str(e) for e in soup.text)\n",
    "    meta_data=json.loads(meta_data)\n",
    "    if(len(meta_data[\"results\"])>0):\n",
    "        for individual in meta_data[\"results\"]:\n",
    "            authors.append([individual[\"id\"],individual[\"display_name\"],individual[\"external_id\"],individual[\"works_count\"],individual[\"hint\"]])\n",
    "            temp=str(individual[\"id\"])+\",\"+str(individual[\"display_name\"])+\",\"+str(individual[\"external_id\"])+\",\"+str(individual[\"works_count\"])+\",\"+str(individual[\"hint\"])+\"\"\n",
    "            file1.write(temp+\"\\n\")\n",
    "print(len(res))\n",
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** The pros of the custom made data, as used in Centola's study, is that he is able to setup the experiment in a very specific way so it is useful for what he is researching. The cons is that there may be some significant bias present, since he is testing for a hypothesis. The results may not be representative of the real world in case the participants know what is going on. Furthermore, creating custom-made data can be time-consuming and resource-intensive.\n",
    "The pros of ready-made data in Nicolaides' study is that he is able to gather a much larger data set at a lower cost. The cons associated with this data is that the purpose of the data may be entirely different, thus the data will not always be of use for all research purposes, e.g. lacking consideration for confounders or other meta data.\n",
    "\n",
    "**2.2** The interpretation of the results in each study can be influenced by the aforementioned pros and cons. Fx. bias has to be considered to a large extent for custom-made data, since as a researcher you collect the data for a hypothesis you have. The confounders may not be considered when ready-made data is created by governments or large corporations to the same extent as custom-made data, thus extra precautions have to be made. In Centola's experiment, the controlled nature of the custom-made data allows for more confident conclusions about causal relationships, while in Nicolaides's study, using ready-made data can provide insights into real-world phenomena and population trends, but you have to be careful in generalizing findings and interpreting results considering the limitations of such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures \n",
    "import requests\n",
    "import math\n",
    "f=open(\"authors.txt\",\"r\",encoding=\"utf-8\")\n",
    "url=\"https://api.openalex.org/works?per-page=200&filter=author.id:\"\n",
    "abstracts=[]\n",
    "papers=[]\n",
    "counters=[]\n",
    "co_authors=[]\n",
    "count=0\n",
    "urls=[]\n",
    "for line in f:\n",
    "    temp=line.split(\",\")\n",
    "    if(temp[3]!=None):\n",
    "        if(int(temp[3])>5 and int(temp[3])<5000):\n",
    "            if(int(temp[3])<201):\n",
    "                temp_url=url+'\"'+temp[0]+'\"'\n",
    "                urls.append(temp_url)\n",
    "            else:\n",
    "                pages=math.ceil(int(temp[3])/200)\n",
    "                for i in range(pages):\n",
    "                    page=i+1\n",
    "                    url_max=\"https://api.openalex.org/works?page=\"+str(page)+\"&per-page=200&filter=author.id:\"+temp[0]+'\"'\n",
    "                    urls.append(url_max)\n",
    "def process_data(url):\n",
    "    r=requests.get(url).json()\n",
    "    for abstract in r[\"results\"]:\n",
    "        if(int(abstract[\"cited_by_count\"])>10 and len(abstract[\"authorships\"])<10):\n",
    "            focuses=[\"Sociology\",\"Psychology\",\"Economics\",\"Political science\"]\n",
    "            quantitative_disciplines=[\"Mathematics\",\"Physics\",\"Computer science\"]\n",
    "            focus=False\n",
    "            quantitative=False\n",
    "            for concept in abstract[\"concepts\"]:\n",
    "                if(int(concept[\"level\"])==0):\n",
    "                    if(concept[\"display_name\"] in focuses):\n",
    "                        focus=True\n",
    "                    if(concept[\"display_name\"] in quantitative_disciplines):\n",
    "                        quantitative=True\n",
    "            for co_author in abstract[\"authorships\"]:\n",
    "                co_authors.append(co_author[\"raw_author_name\"])\n",
    "            if(focus==True and quantitative==True):\n",
    "                abstracts.append([abstract[\"id\"],abstract[\"publication_year\"],abstract[\"cited_by_count\"],temp[0]])\n",
    "                papers.append([abstract[\"id\"],abstract[\"title\"],abstract[\"abstract_inverted_index\"]])\n",
    "    counters.append([\"1\"])\n",
    "    print(len(counters))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(process_data, urls)          \n",
    "abstracts_df=pd.DataFrame(data=abstracts)\n",
    "papers_df=pd.DataFrame(data=papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstracts_df)\n",
    "print(papers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Efficiency in code**: \\\n",
    "Considering the extensiveness of the dataset we work with, we had to implement some strategies for code efficiency. We first retrived the data using the request.get() function. We then looped through all the authors to make sure to remove any duplicates before collecting all unique authors in a txt file. We also made sure to only include authors with a total work count betweenn 5 and 5,000. The most computational-heavy feature was extrating all the works from each author. To speed up this process, we applied multithreading techniques. \\\n",
    "\\\n",
    "**Filtering Criteria and Dataset Relevance**: \\\n",
    "Setting thresholds for authors' total works, citation counts, and relevance to specific fields ensures dataset quality. It includes important authors and influential works while filtering out less significant ones. However, this might exclude things like emerging researchers or niche topics. Regarding the dataset that we have compiled, we could miss out on interdisciplinary research or new methodologies due to emphasis on established topics. Overemphasizing on popular fields could also skew the research focus. Some aspects of Computational Social Science research could be over- or underrepresented as a result of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of papers here\n",
    "weighted_edgelist = []\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "print(G)\n",
    "\n",
    "for authors in papers_df:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ANN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e253e861da49e0bbccda70175bce74f3442aedb410e74d034432e289d13936b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
